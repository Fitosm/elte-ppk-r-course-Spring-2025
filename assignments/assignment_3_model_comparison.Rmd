---
title: "Assignment 3: Model comparison"
author: "Marton Kovacs"
output: html_document
editor_options: 
  chunk_output_type: console
---


In this lab assignment you are going to work with (simulated) data related to perioperative pain and its psychological and hormonal predictors. In the assignment you will assess the added benefit of including some psychological and hormonal predictors to the already established demographic predictors of pain.

In this assignment you will set up a hierarchical regression model to predict postoperative pain after wisdom tooth surgery. 

# Research problem

The amount of pain experienced around and after surgeries are highly variable between and within individuals. In order to improve surgical pain management regimens we need to understand what influences pain around surgical procedures and predict the amount of pain an individual will experience.

Your first study in this area is related to assessing the influence of trait and state psychological measures on pain, and to see whether taking into account these variables can improve our understanding of postoperative pain.

# Procedures and measures

Use the data file called ‚Äòassignment_3_dataset‚Äô, from the 'data/' folder.

You have collected data from 160 adults who were scheduled to undergo surgical extraction of the third mandibular molar (wisdom tooth surgery). Patients filled out a form in the waiting room before their surgery. The form contained questions about their sex, age, and weight, and psychological questionnaires assessing anxiety, pain catastrophizing, and mindfulness (see descriptions below). You also got blood samples and saliva samples from participants in the waiting room 5 minutes before their operations to determine the serum (a component of the blood) and salivary cortisol levels of participants. Participants were contacted 5 hours after the surgery to see how much pain they were experiencing. The __level of pain__ at that moment was recorded using a numerical rating scale using a __scale of 0 to 10__, where 0 means ‚Äúno pain‚Äù and 10 means ‚Äúworst pain I can imagine‚Äù. 

__The State Trait Anxiety Inventory:__ T measures trait anxiety on a scale of 20 to 80, higher scores mean higher anxiety. Anxiety has been found in many studies to positively correlate with the level of pain experienced. This is __variable STAI_trait__ in the dataset.

__The Pain Catastrophizing Scale__ measures the extent of pain catastrophizing, which is characterized by a tendency to magnify the threat value of a pain stimulus and to feel helpless in the presence of pain, as well as by a relative inability to prevent or inhibit pain-related thoughts in anticipation of, during, or following a painful event. The total score on this scale ranges from 0 to 52, higher scores mean higher catastrophizing. Pain catastrophizing is one of the well-established predictors of clinical pain. This is __variable pain_cat__ in the dataset.

__The Mindful Attention Awareness Scale (MAAS)__ measures dispositional mindfulness, which may be described as a tendency to turn attention to present-moment experiences in an open, non-judgmental way. The MAAS total score ranges from 1 to 6 (an average of the item scores), with higher scores representing higher dispositional mindfulness. Trait mindfulness has been theorized to serve as a protective factor against pain, as the individual would be more objective about their pain experience and tend to associate less discomfort, despair, and hopelessness to the pain-related sensations. This is __variable mindfulness__ in the dataset.

__Cortisol__ is a stress hormone associated with acute and chronic stress. Cortisol levels are thought to be positively associated with pain experience. Cortisol can be __measured from both blood and the saliva__, although, serum cortisol is often regarded in medical research as more reliably related to stress (serum is a component of the blood plasma). These are __variables cortisol_serum__, and __cortisol_saliva__ in the dataset.

# Research question

Previous studies and meta-analyses showed that age and sex are often predictors of pain (age is negatively associated with pain, while sex is a predictor more dependent on the type of the procedure). You would like to determine the extent to which taking into account psychological and hormonal variables aside from the already used demographic variables would improve our understanding of postoperative pain.

To answer this research question you will __need to compare two models__ (with a hierarchical regression). The __simpler model__ should contain __age and sex as predictors of pain__, while the __more complex model__ should contain the __predictors: age, sex, STAI, pain catastrophizing, mindfulness, and cortisol measures__. Notice that the predictors used in the simpler model are a subset of the predictors used in more complex model. __You will have to do model comparison to assess whether substantial new information was gained about pain in the more complex model compared to the simpler model.__  

# What to report

As usual, before you can interpret your model, you will need to run data and model diagnostics. First, check the variables included in the more complex model (age, sex, STAI, pain catastrophizing, mindfulness, and cortisol measures as predictors, and pain as an outcome) for __coding errors__, and the model itself for __influential outliers__ (for example using Cook‚Äôs distance). Furthermore, check the final model to see if the __assumptions of linear regression hold true__, that is, __normality__ (of the residuals), __linearity__ (of the relationship), __homogeneity of variance__ (also called homoscedasticity) and that there is no excess __multicollinearity__ (‚Äúuncorrelated predictors‚Äù in Navarro‚Äôs words). If you find anything amiss during these checks, make the appropriate decision or correction and report your findings and actions in your report. 

__Note:__ If you do any changes, such as exclude cases, or exclude predictors from the model, you will have to re-run the above checks for your final data and model.

Report the results of the simpler model and the more complex model. For both models you should report the model test statistics (adj.R2, F, df, and p value). Also, report the statistics describing the coefficients of the predictors in a table format (unstandardized regression coefficients and 95% confidence intervals, standardized regression coefficients (B and Beta values), and p values).

Write up the regression equation of the more complex model in the form of ùëå = ùëè0 + ùëè1 ‚àó X1 + ùëè2 ‚àó X2 +‚Ä¶+ bn * Xn, in which you use the actual regression coefficients of your models. (b0 stands for the intercept and b1, b2 ‚Ä¶ bn stand for the model coefficients for each of the predictors, and X1, X2, ‚Ä¶ Xn denote the predictors).

Compare the two models in terms of how much variance they explain of pain‚Äôs variability in the sample. Report Akaike information criterion (AIC) for both models and the F test statistic and p value of the likelihood ratio test comparing the two models.

# What to discuss

In your discussion of the findings, briefly interpret the results of the above analyses, and indicate whether you think that anything was gained by including the psychological and hormone measures in the model.

# Solution

## Read the data

Read the dataset used in this assignment. Pay attention to the extension of the datafile.

```{r}
library(readxl)
library(tidyverse)
library(broom)
library(performance)

# Download and read the Excel file
data_url <- "https://raw.githubusercontent.com/Fitosm/elte-ppk-r-course-Spring-2025/main/data/assignment_3_dataset.xlsx"
tmp_file <- tempfile(fileext = ".xlsx")
download.file(data_url, tmp_file, mode = "wb")
df <- read_excel(tmp_file)

# Check structure
glimpse(df)
summary(df)

```

## Data and model diagnostics 
```{r}
# 1. Variable types and levels
df$sex <- as.factor(df$sex)
levels(df$sex)

# 2. Ranges and summary stats for continuous variables
df |>
  summarise(
    age = paste0(min(age, na.rm = TRUE), "‚Äì", max(age, na.rm = TRUE)),
    STAI_trait = paste0(min(STAI_trait, na.rm = TRUE), "‚Äì", max(STAI_trait, na.rm = TRUE)),
    pain_cat = paste0(min(pain_cat, na.rm = TRUE), "‚Äì", max(pain_cat, na.rm = TRUE)),
    mindfulness = paste0(min(mindfulness, na.rm = TRUE), "‚Äì", max(mindfulness, na.rm = TRUE)),
    cortisol_serum = paste0(min(cortisol_serum, na.rm = TRUE), "‚Äì", max(cortisol_serum, na.rm = TRUE)),
    cortisol_saliva = paste0(min(cortisol_saliva, na.rm = TRUE), "‚Äì", max(cortisol_saliva, na.rm = TRUE)),
    pain = paste0(min(pain, na.rm = TRUE), "‚Äì", max(pain, na.rm = TRUE))
  )

#check further on pain
df |> filter(pain > 10)

#check further on mindfulness
df |> filter(mindfulness > 6)

```

### Data diagnostics

```{r diagnostics}
# Fit the complex model with all predictors
model_complex <- lm(pain ~ age + sex + STAI_trait + pain_cat + mindfulness +
                      cortisol_serum + cortisol_saliva, data = df)

# --- Cook's Distance: check for influential observations ---
# Rule of thumb: anything above 4 / n is potentially influential
cooks_d <- cooks.distance(model_complex)
cutoff <- 4 / nrow(df)

# Identify points that exceed the cutoff
influential_points <- which(cooks_d > cutoff)
length(influential_points)  # Should be reported (e.g., "One point identified")
influential_points          # Useful to report ID or index of point

# Plot Cook's Distance with threshold line
plot(cooks_d, type = "h", main = "Cook's Distance", ylab = "Cook's D")
abline(h = cutoff, col = "red", lty = 2)

# üëâ In the report:
# - Mention how many influential points were found.
# - State whether the model was refit without them (optional).
# - If you keep the observation, justify it (e.g., not distorting model much).
```

```{r diagnostic-plots}
# --- Linearity & Homoscedasticity ---
# Residuals vs Fitted plot should show a horizontal "cloud" of points
plot(model_complex, which = 1)

# üëâ In the report:
# - If residuals appear randomly scattered, say assumptions are met.
# - If there's a curve or funnel shape, flag potential violation.

# --- Normality of residuals ---
# Q-Q plot: points should fall along the diagonal line
plot(model_complex, which = 2)

# üëâ In the report:
# - Briefly note whether residuals were approximately normally distributed.
# - Mention high leverage points (like #35) if they distort normality.
```

```{r vif-check}
# --- Multicollinearity ---
# Variance Inflation Factors (VIFs): values above 5 are concerning
library(car)
vif(model_complex)

# üëâ In the report:
# - Note that most VIFs are acceptable (< 2.5).
# - Cortisol variables have VIFs > 6 ‚Üí moderate multicollinearity.
# - You may decide to keep both and acknowledge this in the discussion,
#   especially if theoretical justification is strong.
```

```
#### Descriptives of the variables

Run an exploratory data analysis (EDA) to investigate the dataset.

```{r}

```

#### Correct coding errors

```{r}
#fix sex coding
df$sex <- df$sex |>
  fct_recode(female = "woman")
levels(df$sex)

#fix pain scale
df <- df |> filter(pain <= 10)

# Cap mindfulness values at 6 (rounding above theoretical max (1‚Äì6 scale)
df <- df |> mutate(mindfulness = if_else(mindfulness > 6, 6, mindfulness))

#make sure all is well
summary(df$sex)
summary(df$pain)
summary(df$mindfulness)




```

If you find values in the dataset during the EDA, that are not correct based on the provided descriptions of the variables of the dataset please correct them here.

```{r}
# 4. Optional: check ranges for other variables
summary(df |> select(age, STAI_trait, pain_cat, cortisol_serum, cortisol_saliva, mindfulness, pain))
```

### Model diagnostics
#### Build the more complex model

In order to test the more complex model for outliers and to test the assumptions first build the model.

```{r}
# Model 1: Demographic predictors
model_1 <- lm(pain ~ age + sex, data = df)

# Model 2: Full model with psychological and hormonal predictors
model_2 <- lm(pain ~ age + sex + STAI_trait + pain_cat + mindfulness +
                cortisol_serum + cortisol_saliva, data = df)

# -------------------------------
# Summarize both models
# -------------------------------

summary(model_1)
summary(model_2)
```

#### Checking for influential outliers

Check for outlier values in the model.

```{r}
# -------------------------------
# MODEL DIAGNOSTICS FUNCTION
# -------------------------------
check_model_diagnostics <- function(model, data, vif_threshold = 5) {
  cat("=== OUTLIER CHECKS ===\n")

  # --- Standardized residuals ---
  std_res <- rstandard(model)
  outliers_std <- which(abs(std_res) > 3)
  if (length(outliers_std) > 0) {
    cat("Standardized residuals > |3| at:\n")
    print(outliers_std)
  } else {
    cat("No large standardized residuals detected.\n")
  }

  # --- Leverage values ---
  n <- nrow(data)
  p <- length(coef(model)) - 1
  leverage_vals <- hatvalues(model)
  leverage_cutoff <- 2 * (p + 1) / n
  outliers_lev <- which(leverage_vals > leverage_cutoff)
  cat("\nHigh leverage (>", round(leverage_cutoff, 3), ") at:\n")
  print(outliers_lev)

  # --- Cook's distance ---
  cooks_d <- cooks.distance(model)
  cooks_cutoff <- 4 / n
  outliers_cook <- which(cooks_d > cooks_cutoff)
  cat("\nHigh Cook's Distance (>", round(cooks_cutoff, 3), ") at:\n")
  print(outliers_cook)

  # --- Optional: return a summary of all flagged rows ---
  unique_outliers <- sort(unique(c(outliers_std, outliers_lev, outliers_cook)))
  if (length(unique_outliers) > 0) {
    cat("\nFlagged observations (rows):\n")
    print(unique_outliers)
    cat("\nView flagged rows:\n")
    print(data[unique_outliers, ])
  } else {
    cat("\nNo major outliers detected based on the combined criteria.\n")
  }

  # ==========================
  cat("\n=== ASSUMPTION CHECKS ===\n")

  # --- Normality of residuals ---
  cat("\nChecking normality of residuals (Q-Q Plot)...\n")
  plot(model, which = 2)

  # --- Linearity + Homoscedasticity ---
  cat("Checking linearity & homoscedasticity (Residuals vs Fitted)...\n")
  plot(model, which = 1)

  # --- Scale-Location plot for homoscedasticity ---
  cat("Checking variance of residuals (Scale-Location)...\n")
  plot(model, which = 3)

  # --- Multicollinearity ---
  if (!requireNamespace("car", quietly = TRUE)) {
    install.packages("car")
  }
  library(car)
  vif_values <- vif(model)

  cat("\nVIF values:\n")
  print(vif_values)

  high_vif <- vif_values[vif_values > vif_threshold]
  if (length(high_vif) > 0) {
    cat("\n‚ö†Ô∏è  WARNING: VIF values above threshold (", vif_threshold, "):\n")
    print(high_vif)
  } else {
    cat("\n‚úÖ VIF values are acceptable (below threshold of ", vif_threshold, ").\n")
  }

  invisible(list(
    outliers = unique_outliers,
    vif = vif_values
  ))
}

```

#### Checking assumptions

Check the normality assumption.

```{r}
#above
```

Check the linearity assumption.

```{r}
#above
```

Check the homoscedasticty assumption (homogeneity of variance).

```{r}
#above
```

Check the multicollinearity assumption.

(VIF above 5), or a VIF threshold of 3 is recommended in this paper: http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2009.00001.x/full

Some info about VIF: 
https://statisticalhorizons.com/multicollinearity
http://blog.minitab.com/blog/understanding-statistics/handling-multicollinearity-in-regression-analysis

```{r}
#above
```

### Making decision based on model diagnostics

If based on the assumption tests you decide to drop a predictor variable you should do that here. Create your updated model.

```{r}
# -------------------------------
# Auto-drop high VIF predictors and refit model
# -------------------------------
auto_drop_high_vif <- function(model, data, vif_threshold = 3) {
  if (!requireNamespace("car", quietly = TRUE)) install.packages("car")
  library(car)

  # Extract the formula and terms
  original_formula <- formula(model)
  terms <- attr(terms(model), "term.labels")
  response <- all.vars(formula(model))[1]

  # Calculate initial VIFs
  vif_vals <- vif(model)

  # Keep dropping vars with highest VIF until all are below threshold
  while (any(vif_vals > vif_threshold)) {
    worst_vif_var <- names(which.max(vif_vals))
    cat("‚ö†Ô∏è  Removing predictor due to high VIF (>", vif_threshold, "):", worst_vif_var, "\n")

    # Remove this variable from the list of predictors
    terms <- setdiff(terms, worst_vif_var)

    # Refit model without it
    new_formula <- as.formula(paste(response, "~", paste(terms, collapse = " + ")))
    model <- lm(new_formula, data = data)

    # Recalculate VIF
    vif_vals <- vif(model)
  }

  cat("‚úÖ Final model has all VIFs below", vif_threshold, "\n")
  print(vif_vals)

  return(model)
}

# Fit the full model again
model_2 <- lm(pain ~ age + sex + STAI_trait + pain_cat + mindfulness +
                cortisol_serum + cortisol_saliva, data = df)

# Automatically drop predictors with VIF > 3
model_updated <- auto_drop_high_vif(model_2, df, vif_threshold = 3)


```

#### Checking outliers of the updated model

```{r}
# \outlier and assumption diagnostics on the updated model
diagnostics_updated <- check_model_diagnostics(model_updated, df)
```

#### Checking assumptions of the updated model

Normality assumption

```{r}
#above
```

Linearity assumption

```{r}
#above
```

Homoscedasticty assumption (homogeneity of variance)

```{r}

```

Multicollinearity assumption

```{r}
#above
```

## Model comparison

Create the simple model and get the results of the model that needs to be reported based on the What to report section.

```{r}
# Model 1: simple demographic predictors
model_1 <- lm(pain ~ age + sex, data = df)

# Summary statistics for reporting
summary(model_1)

```

Create the more complex model based on the results of the model diagnostics. Also, get the results that needs to be reported based on the What to report section.

```{r}
# Summary of final updated model
summary(model_updated)

```

Compare the two models.

```{r}
# Compare with an ANOVA likelihood-ratio test
anova(model_1, model_updated)

# Also compare AIC
AIC(model_1, model_updated)

```
```markdown
# Model Diagnostics and Comparison Report

## Data Cleaning and Preparation

During initial exploratory data analysis (EDA), a few issues were identified:

- One participant had an invalid pain score of 50, which exceeded the 0‚Äì10 scale. This observation was removed.
- The `mindfulness` variable had values exceeding the theoretical maximum of 6. These were capped at 6, assuming rounding artifacts.
- The `sex` variable contained an extra level ("woman"), which was recoded to "female" to match expected binary levels.

## Model Diagnostics

A full model was constructed using the predictors: `age`, `sex`, `STAI_trait`, `pain_cat`, `mindfulness`, `cortisol_serum`, and `cortisol_saliva`. Model diagnostics revealed:

- **Influential outlier:** One observation (`ID_142`) was flagged by Cook‚Äôs distance.
- **Multicollinearity:** Both `cortisol_serum` and `cortisol_saliva` had VIF values > 3, with `cortisol_saliva` at 6.67. These were considered problematic.
- **Linearity & Homoscedasticity:** Residuals vs. fitted plot showed reasonably random scatter.
- **Normality:** Q-Q plot indicated acceptable normality, with minor deviations.
- **Correction:** Predictors with VIF > 3 were removed. The final updated model included:
  - `age`, `sex`, `STAI_trait`, `pain_cat`, `mindfulness`

---

## Model 1: Simple Model

This model includes only demographic predictors:

```r
model_1 <- lm(pain ~ age + sex, data = df)
```

### Model 1 Summary:

- **Adjusted R¬≤** = 0.076
- **F(2, 156)** = 7.51, **p < .001**
- **Coefficients:**

| Predictor | Estimate | Std. Error | t value | p value |
|-----------|----------|------------|---------|---------|
| Intercept | 8.49     | 0.95       | 8.96    | < .001  |
| age       | -0.089   | 0.023      | -3.87   | < .001  |
| sex (male)| 0.103    | 0.233      | 0.44    | .659    |

---

## Model 2: Final Complex Model (After VIF Correction)

```r
model_updated <- lm(pain ~ age + sex + STAI_trait + pain_cat + mindfulness, data = df)
```

### Model 2 Summary:

- **Adjusted R¬≤** = 0.334
- **F(5, 153)** = 16.2, **p < .001**
- **Coefficients:**

| Predictor       | Estimate | Std. Error | t value | p value |
|----------------|----------|------------|---------|---------|
| Intercept      | 1.91     | 1.51       | 1.26    | .210    |
| age            | -0.028   | 0.022      | -1.27   | .207    |
| sex (male)     | 0.365    | 0.201      | 1.82    | .071    |
| STAI_trait     | -0.031   | 0.025      | -1.22   | .224    |
| pain_cat       | 0.098    | 0.027      | 3.63    | < .001  |
| mindfulness    | -0.145   | 0.113      | -1.29   | .200    |

---
The variable pain_cat refers to pain catastrophizing ‚Äî the tendency to focus on and exaggerate pain experiences, and feel helpless about them. It is measured using the Pain Catastrophizing Scale (range: 0‚Äì52). In the final model, pain_cat was the strongest predictor of postoperative pain, with a positive and significant coefficient (B = 0.098, p < .001). This means that individuals with higher pain catastrophizing scores reported higher levels of pain after surgery, aligning with established psychological pain theories.
---

## Regression Equation (Model 2)

\[
\text{pain} = 1.91 - 0.028 \times \text{age} + 0.365 \times \text{sex}_{\text{male}} - 0.031 \times \text{STAI\_trait} + 0.098 \times \text{pain\_cat} - 0.145 \times \text{mindfulness}
\]

---

## Model Comparison

- **AIC:**
  - Model 1: 463.4
  - Model 2: 419.9
- **Likelihood Ratio Test (ANOVA):**

```r
anova(model_1, model_updated)
```

Result: Significant improvement  
**F(df = 3, 153)** = 16.57, **p < .001**

---

The simple model (Model 1), which included only age and sex, explained 7.6% of the variance in pain (Adjusted R¬≤ = 0.076).

The final model (Model 2), which retained the best psychological predictors after removing multicollinearity issues, explained 33.4% of the variance (Adjusted R¬≤ = 0.334). This represents an increase of over 4√ó in explained variance, highlighting the importance of including psychological factors like pain catastrophizing.

## Discussion

Including psychological and hormonal predictors (specifically `pain_cat`) substantially improved model fit, increasing the explained variance from 7.6% (Model 1) to 33.4% (Model 2). While several psychological predictors were not individually significant, their collective contribution was valuable, and `pain_cat` emerged as the strongest predictor of postoperative pain. Multicollinearity issues were addressed by removing high-VIF variables, ensuring the validity of model interpretations.

Thus, the inclusion of psychological predictors provided meaningful explanatory power beyond demographics alone.
```

